% Basic document and typesetting settings
\documentclass[a4paper, twoside, 11pt]{report}
\usepackage{glossaries}

\newglossary{symbols}{sym}{sbl}{List of Abbreviations and Symbols}
\makeglossary

\newglossaryentry{var}{
type=symbols,
name={\ensuremath{X_i}},
description={BN variable}
}

\newglossaryentry{var_set}{
type=symbols,
name={\ensuremath{\mathcal{X}}},
description={set of all variables in the BN}
}

%\usepackage{geometry}
\usepackage[english]{babel}
\usepackage[hidelinks]{hyperref}
\usepackage{comment}
\usepackage{cite}
\usepackage{adjustbox}
\usepackage{enumitem}
	\setenumerate{itemsep = 0.1cm}
\usepackage[font = small, margin=0.5cm]{caption}
\usepackage{subcaption}
%\usepackage{fullpage} % messes up the margins headers/footers
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage[section]{placeins}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{lipsum}
\usepackage[hang,flushmargin]{footmisc} 
\usepackage{cleveref}
\usepackage{multicol}
\usepackage[numbers,square,sort]{natbib}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{float} 
\usepackage{cancel}
\usepackage{multirow} 
\usepackage{booktabs} 
\usepackage{varioref} 
\usepackage{filecontents}
\usepackage[outdir=./]{epstopdf}
\usepackage{tabto}
\NumTabs{4}
\usepackage{wrapfig}
\usepackage{pgf}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{colorlinks, breaklinks, urlcolor=black, linkcolor=black, citecolor=black}
\usepackage{pgfplots}
\usepackage{array}
\usetikzlibrary{shapes, arrows}
\tikzset{
    events/.style={ellipse, draw, align=center},
}
\usepackage{filecontents}
\setlength\parindent{24pt}
\usepackage[hmarginratio=1:1, left=25mm, top=25mm, bottom=30mm]{geometry}

\AtBeginDocument{\renewcommand{\bibname}{References}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

%Font and spacing of words/lines
\setlength{\textfloatsep}{5pt}
\usepackage{libertine}
\usepackage{setspace}
	\setstretch{1.05}
\usepackage[tracking = true, letterspace = 100]{microtype}

%Sectioning settings
\usepackage{abstract}
	\renewcommand{\abstractnamefont}{\scshape\sffamily\Large}
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
	
\usepackage{titlesec}
	\titleformat	
		\chapter[display]
			\huge
  				{\textsc{\lsstyle\chaptertitlename\ \thechapter}}{0pt}{\Huge\bfseries}
	\titleformat
		\section
  			{\bfseries\normalfont\Large}{\thesection}{1em}{}
	\titleformat
		\subsection
			{\bfseries\normalfont\large}{\thesubsection}{1em}{}
	\titlespacing*{\section}{0cm}{1pc}{1pc}
	
\titleformat{\chapter}
  {\bfseries\huge}{\normalfont{\lsstyle\chaptertitlename\ \thechapter}}{1em}{}
  
\titlespacing*{\chapter}{0pt}{0pc}{1.5pc}
\titlespacing*{\section}{0pt}{1.5pc}{.5pc}
\titlespacing*{\subsection}{0pt}{1.5pc}{.5pc}

%Headers and footers
\usepackage{fancyhdr}
	\setlength{\headheight}{13.59999pt}
	\pagestyle{plain}
		{
			\fancyhf{}
			\renewcommand{\headrulewidth}{0pt}
			\fancyfoot[C]{\thepage}
		}
	\pagestyle{fancy}
		{
			\fancyhf{}
			\renewcommand{\sectionmark}[1]{\markright{\thesection~ - ~#1}}
			\renewcommand{\chaptermark}[1]{\markboth{\chaptername~\thechapter~ - ~#1}{}}
			\fancyhead[LO]{\nouppercase{\textsc\rightmark}}
			\fancyhead[RE]{\nouppercase{\textsc\leftmark}}
			\fancyfoot[C]{\thepage}
		}


%Pictures
\usepackage{graphicx}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Tikz
\usepackage{tikz}
	\usetikzlibrary{positioning}
    \usetikzlibrary{calc}
\usepackage{tikz-qtree}
\usetikzlibrary{arrows,calc,plotmarks,intersections}
\tikzset{>=stealth', help lines/.style={dashed, thick}, axis/.style={<->}, important line/.style={thick}, connection/.style={thick, dotted},}
\usetikzlibrary{shapes.geometric,positioning}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathrsfs}
	
%Theorem commands
\theoremstyle{plain}
	\newtheorem{thm}{Theorem}[chapter]
	\newtheorem{lem}[thm]{Lemma}
	\newtheorem{con}[thm]{Conjecture}
\theoremstyle{definition}
	\newtheorem{definition}[thm]{Definition}
	\newtheorem{example}[thm]{Example}
	\newtheorem{exmps}[thm]{Examples}
	\newtheorem{prop}[thm]{Proposition}
\theoremstyle{remark}
	\newtheorem*{remark}{Remark}
	\newtheorem*{remarks}{Remarks}
	\newtheorem{notation}[thm]{Notation}


% Probability notations
% Definition commands

\newcommand{\A}{{\mathcal A}}
\newcommand{\B}{{\mathcal B}}
\newcommand{\C}{{\mathcal C}}
\newcommand{\E}{{\mathbb E}}
\newcommand{\F}{{\mathcal F}}
\newcommand{\G}{{\mathcal G}}
\renewcommand{\H}{{\mathcal H}}
\newcommand{\I}{{\mathcal{I}}}
\newcommand{\M}{{\mathcal M}}
\newcommand{\Q}{{\mathcal{Q}}}
\newcommand{\U}{{\mathcal{U}}}
\newcommand{\X}{{\mathcal{X}}}

\newcommand{\calP}{{\mathcal{P}}}
\newcommand{\Var}{{\text{Var}}}

\newcommand{\one}[1]{\mathrm{#1}}
\newcommand{\two}[2]{\mathrm{#1#2}}
\newcommand{\three}[3]{\mathrm{#1#2#3}}
\newcommand{\four}[4]{\mathrm{#1#2#3#4}}

\newcommand{\ps}{\textit{prune sampling }}
\newcommand{\Ps}{\textit{Prune sampling }}

\renewcommand{\gg}{{\text{g} }}
\newcommand{\gT}{{\text{g}_\text{T} }}
\newcommand{\gF}{{\text{g}_\text{F} }}
\renewcommand{\ss}{{\text{s} }}
\newcommand{\sT}{{\text{s}_\text{T} }}
\newcommand{\sF}{{\text{s}_\text{F} }}
\newcommand{\rr}{{\text{r} }}
\newcommand{\rT}{{\text{r}_\text{T} }}
\newcommand{\rF}{{\text{r}_\text{F} }}


%mathbf
\newcommand{\bfe}{{\mathbf{e}}}
\newcommand{\bfx}{{\mathbf{x}}}
\newcommand{\bfy}{{\mathbf{y}}}
\newcommand{\bfX}{{\mathbf{X}}}
\newcommand{\bfE}{{\mathbf{E}}}

%color-text
\newcommand{\red}[1]{{\textcolor{red}{#1}}}

% number systems

\def\N{{\mathbb N}}
\def\bbZ{{\mathbb Z}}
\def\bbQ{{\mathbb Q}}
\def\bbR{{\mathbb R}}
\def\bbC{{\mathbb C}}

\renewcommand{\P}{{\mathbb P}}

% Renewed definition commands
\renewcommand{\S}[1]{\mathscr{#1}}

% Renewed commands
\renewcommand{\epsilon}{\varepsilon}

\begin{document}

%\newgeometry{hmarginratio = 1:1}


%% new commands

%independent
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

% Titlepage

\begin{titlepage}
	\centering
	\vspace{1cm}
	
\scshape\sffamily{\Large master thesis jurriaan parie }\\[1.5cm]

\HRule \\[0.4cm]
{ \huge \scshape\sffamily{ performance of sampling methods \vskip 0.25cm for deterministic bayesian networks}} \\[0.3cm]
\HRule \\[1.5cm]

\scshape\sffamily{ \Large msc mathematical sciences, \\ utrecht university }

\vfill
	
\large \scshape\sffamily{supervised by:}

\vspace{2pc}
	
\noindent\makebox[0.5\linewidth][c]{%
\begin{minipage}[t!]{.47\textwidth}
	{\large \scshape\sffamily{dr. frank philipson} } \\ \\
	\scshape\sffamily{cyber security and robustness, tno} \\
	\centering
	\includegraphics[scale=0.54]{tno_logo_zwart.jpg}
\end{minipage}}
\hfill
\begin{minipage}[t!]{.47\textwidth}
	{ \large \scshape\sffamily{prof. dr. gerard barkema} } \\ \\
	\scshape\sffamily{information and computing sciences, utrecht university}
	\centering
	\includegraphics[height = 7pc]{UU_logo_EN_RGB.jpg}
\end{minipage}

\vfill

\large \scshape\sffamily{september 14, 2018}

	
\end{titlepage}


\pagenumbering{roman}
%\pagenumbering{arabic}

\pagestyle{plain}

% Abstract

\begin{abstract}
The performance of the recently introduced \ps algorithm \cite{phillipson2018} is characterised for various types of Bayesian networks and compared to the popular Markov chain Monte Carlo (MCMC) sampling methods Gibbs- and Metropolis sampling. We devised a procedure to obtain the performance of the MCMC sampling methods in the limit of infinite simulation time, extrapolated from relatively short simulations. This approach was used to conduct a study to compare the accuracy, rate of convergence and the time consumption of the sampling methods. We show that Markov chains created by \textit{prune sampling} always converge to the desired posterior distribution, also for networks where conventional Gibbs sampling fails. {Beside that, we demonstrate} that \ps outperforms {Gibbs sampling -- arguably the most widely used MCMC inference technique --} at least for a class of BNs. Though, this tempting feature comes at a price. In the first version of \textit{prune sampling}, the procedure to choose a configuration of the BN for the next iteration is rather time intensive. Our conclusion is that \ps is {a competitive} method for all types of small and medium sized BNs, but standard methods perform better for all types of large BNs. Ultimately, we improved  the first version of \ps by providing sophisticated solutions for its flaws.

\blfootnote{\noindent Part of this thesis will be published as \textit{Prune sampling: a MCMC inference technique for discrete and deterministic Bayesian networks}} 
\end{abstract}

%a competitive
%former: the preferred

%Gibbs sampling
%former: standars sampling techniques

%Beside that, we demonstrate that
%former: We show

%In the first version of \ps, for large BNs the procedure to choose the next iteration step uniformly is rather time intensive.
%former: In the first version of \ps, the procedure to choose uniformly a configuration for large BNs for the next iteration is rather time intensive.

% Contents

\tableofcontents

\clearpage

\pagenumbering{arabic}

\restoregeometry


%Introduction

\chapter{Introduction}\label{intro}
This thesis is about Bayesian network inference. In particular, it is about the performance of Markov chain Monte Carlo methods in presence of deterministic relations, which form an important class of approximate inference techniques for Bayesian networks. In this first chapter, we give a brief overview of the study we performed in this master thesis.

\section{Bayesian networks and inference}
A Bayesian network (BN) is a probabilistic model that represents a set of random variables and their conditional dependencies. One could represent a BN graphically by considering a directed acyclic graph where the set of nodes is induced by the set of random variables and where the set of edges is given by the conditional dependencies between these random variables. Assuming that instances fall into one of a number of mutually exclusive and exhaustive classes, discrete BNs are used to model probabilistic relationships. As an illustration, BNs model genetic linkage \cite{fishelson2004}, causal reasoning \cite{pearl2014probabilistic} and defence systems \cite{phillipson2015modelling}. For all of these models, BNs are used to answer probabilistic queries about variables and their relationships. An interesting feature of BNs is that the network can be used to find out updated knowledge of the state of a subset of variables when other variables (the evidence variables) are observed. This process, of computing the posterior distribution of variables when evidence is given, is called inference. Though, exact inference in BNs is often too computationally intensive. On the other hand, approximate inference methods have to deal with a lack of convergence and often perform poorly in the presence of determinism \cite{koller2009probabilistic, poon2006sound, gogate2011samplesearch}. Therefore, a plethora of different inference strategies have been developed \cite{nasrabadi2007pattern, nielsen2009bayesian, koller2009probabilistic, pearl2014probabilistic}.

\section{Goals and approach of this research project}
In order to improve the reliability of approximate inference methods, at TNO an unprecedented Markov Chain Monte Carlo (MCMC) approximate inference method named \ps was created. In this research project, I characterise the performance of the first implemented version of \ps for discrete and deterministic BNs. We devised a procedure to obtain the performance of MCMC sampling methods in the limit of infinite simulation time, extrapolated from relatively short simulations. This approach was used to conduct a study to compare the accuracy, rate of convergence and the time consumption of \ps with two conventional MCMC sampling methods: Gibbs- and Metropolis sampling. In addition, we disclose the pitfalls of \ps and show how this first version could be improved. Results of this study will be published in \textit{Prune sampling: a MCMC inference technique for discrete and deterministic Bayesian networks} \cite{phillipson2018}.


\section{Overview of the thesis}
We start introducing the BN framework, inference techniques and their shortcomings in Chapter 2. In Chapter 3, we introduce the concept and elaborate on the implementation of \textit{prune sampling}. Consecutively, in Chapter 4, we present the performance of \ps in terms of accuracy, the rate of convergence and time consumption compared with Metropolis- and Gibbs sampling. In Chapter 5, we propose solutions to the flaws of \ps and and discuss results of improved versions of the algorithm.

\chapter{Bayesian network inference}
In this chapter, we introduce the main concepts our study: the framework of Bayesian networks (BNs), the task of doing BN inference and Markov chain Monte Carlo (MCMC) sampling methods for BNs. We highlight the pros and cons of commonly used MCMC inference techniques. Such that -- later on -- we can make a comparison between the performance of those methods and the \ps method more clearly.

\section{Bayesian networks}
First, we introduce the concept of a BN structure and its corresponding probabilistic methodology.
\begin{definition}[Bayesian network]
A BN structure $\G$ is a directed acyclic graph whose nodes represent random variables $\X = (X_1, \ldots , X_n)$. Let $\text{Pa}_{X_i}$ denote the parents of $X_i$ in $\G$ and $\text{ND}_{X_i}$ denote the variables in the graph that are not descendants of $X_i$. Then $\G$ encodes the following set of conditional independence assumptions, called the local independencies, and denoted by $\I_l(\G)$:
\begin{align*}
\text{for each variable $X_i$: $(X_i \indep \text{ND}_{X_i} | \text{Pa}_{X_i})$}.
\end{align*}
In other words, the local independencies state that each node $X_i$ is conditionally independent of its non-descendants given its parents \citep[p.~57]{koller2009probabilistic}.
\end{definition}

When dealing with spaces composed solely of discrete-valued random variables, to each node $X_i$ we assign a state $x_i \in Val(X_i)$. Here, $Val(X_i)$ denotes the set of values that a random variable $X_i$ can take. We display the conditional probability distribution $P(X_i | \text{Pa}_{X_i})$ in a conditional probability table (CPT), where
\begin{align*}
\sum_{i \in \{1, \ldots , n\}} P(x_i | \text{Pa}_{X_i}) = 1.
\end{align*}
So, a BN exists of a graph with a collection of local probability distributions, given in CPTs. Together, these local probability distributions give the joint probability distribution of the BN. We use $\bfX \subseteq \X$ to denote a set of random variables, while $\bfx$ denotes an assignment of values to the variables in this set. For convenience, a state (or configuration) of a BN is denoted as $\bfx = (x_1, \ldots , x_n)$ and $P(\bfx)$ denotes the probability of the BN having this state $\bfx$. Next, we introduce the event called determinism \citep[p.~158]{koller2009probabilistic}.
\begin{definition}[Deterministic relation]
The CPT contains one of more zeros. That is, there exists a function $f: Val(\text{Pa}_{X_i}) \to Val(X_i)$, such that
\begin{align*}
P(x_i | \text{Pa}_{X_i}) =
\begin{cases}
1 & x_i = f(\text{Pa}_{X_i}) \\ 
0 & \text{otherwise}.
\end{cases}
\end{align*} 
\end{definition}
The above introduced concepts are all depicted in Example \ref{ex:rain-sprinkler}.
\begin{example}\label{ex:rain-sprinkler}
Consider the \textit{Rain-Sprinkler} BN in Figure \ref{rainBN}. In this model, one could consider the event of grass being wet ($\gg$) as a results of two causes: either a sprinkler ($\ss$) is on or it is raining ($\rr$). Beside that, it is supposed that the rain has a direct effect on the use of the sprinkler. All three variables have two possible values, T (for true) and F (for false). From the CPTs it becomes clear that when it rains, the sprinkler is usually not turned on, i.e. $P( \sT |  \rT) = 0.01$. The counter intuitive state of the BN, that the grass is wet given it is not raining and the sprinkler is off: $\bfx = (\rF,  \sF, \gT)$, is excluded by the deterministic relation $P(\gT | \rF,  \sF) = 0$.  Taking the fixed (topological) order of the variables $R, S, G$ into account, the joint probability function is given by
\begin{align}
P(R, S, G) = P(G | R, S) P(S|R) P(R).
\label{eq:jointprob}
\end{align}
This BN can answer queries like \textit{what is the probability that it is raining, given the grass is wet?} by using the conditional probability formula and summing over all (auxiliary) variables
\begin{align}
P(R = \rT | G = \gT) = \frac{P(R = \rT, G = \gT)}{P(G = \rT)} = \frac{\sum_{S \in \{T,F\}} P(R = \rT, S, G = \gT)}{\sum_{R, S \in \{T,F\}} P(R, S, G = \gT)}.
\label{eq:conditional-summation}
\end{align}
\end{example}
More background concerning the mathematical representation of BNs and its methodology could be found in \cite{koller2009probabilistic}, Chapter~3; \cite{nielsen2009bayesian}, Chapter~2; \cite{pearl2014probabilistic}, Chapter~2. 
\begin{center}
\begin{figure}[t!]
\centering
\begin{tikzpicture}[node distance=1.2cm, >=triangle 60]
\node [events] (s) {Sprinkler};
\node [events,  right=of s] (r) {Rain};
\node [events,  below right=of s] (g) {Grass wet};


\draw [->] (r) -- (s);
\draw [->] (r) -- (g);
\draw [->] (s) -- (g);

{\small
\node [right = 2cm of r] (t_r) {
    \begin{tabular}{|c||c|} \hline
    	$\text{r}_\text{T}$ & $0.2$\\ \hline
    	$\text{r}_\text{F}$ & $0.8$ \\ \hline
    \end{tabular}
    };
    
\node [left = 2cm of s] (t_s) {
    \begin{tabular}{|c||c|c|} \hline
	& $\text{r}_\text{T}$ & $\text{r}_\text{F}$    \\ \hline \hline
	$\text{s}_\text{T}$ & 0.01 & 0.4   \\ \hline
	$\text{s}_\text{F}$ & 0.99 & 0.6 \\ \hline
\end{tabular}
    };
    
\node [below left = 1cm of g] (t_g) {
    \begin{tabular}{|c||c|c|c|c|} \hline
	& $\text{r}_\text{T},\ \text{s}_\text{T}$ & $\text{r}_\text{T},\ \text{s}_\text{F}$ & $\text{r}_\text{F},\ \text{s}_\text{T}$ & $\text{r}_\text{F},\ \text{s}_\text{F}$  \\ \hline \hline
	$\text{g}_\text{T}$ & 0.99 & 0.8 & 0.9 & 0  \\ \hline
	$\text{g}_\text{F}$ & 0.01 & 0.2 & 0.1 & 1 \\ \hline
\end{tabular}
    };
}
    
\draw [dotted] (r) -- (t_r);
\draw [dotted] (s) -- (t_s);
\draw [dotted] (g) -- (t_g);

\end{tikzpicture}
\caption{a BN structure and corresponding CPTs are used to model the event of grass being wet (g) as a results of two causes: either a sprinkler (s) is on or it is raining (r).}
\label{rainBN}
\end{figure}
\end{center}
\vspace{-1.9pc}
\section{Inference}
The task of answering types of questions as \textit{what is $P(R = \text{r}_\text{T} | G = \text{g}_\text{T})$?} is called \textit{inference}. Below, we describe how one could answer such queries by computing the exact probability. \\

When values of variables are known, or are \textit{given}, we call this set $\bfE \subset \X$ evidence. Now, we can formulate the main goal of our this study as: given a set of evidence $\bfE = \bfe$ and nodes of interest $\bfX \subset \X$, such that $\bfE \cap \bfX = \emptyset$, what is the probability distribution $P(\bfX | \bfE = \bfe)$? Answering this type of questions is called \textit{inferring unobserved variables}. We write $P$ as the posterior probability distribution of interest, with reduced CPTs according to the evidence nodes. In the next example, we return to the \textit{Rain-Sprinkler} BN to give an illustration of this type of inference.
\begin{example}\label{ex:exact-marg}
Again, consider the BN from Figure \ref{rainBN}. We show how the question in Example \ref{ex:rain-sprinkler} -- \textit{what is $P(R = \text{r}_\text{T} | G = \text{g}_\text{T})$?} -- could be computed explicitly. Using the expansion for the joint probability function from Equation \ref{eq:jointprob} and the conditional probabilities from the CPTs, one could evaluate each term in the numerator and denominator of Equation \ref{eq:conditional-summation} in the way we evaluate
\begin{align*}
P(R = \rT, S = \sF, G = \gT) &= P(G = \gT | R = \rT, S = \sF) P(S = \sF | R = \rT) P(R = \rT) \\
&= 0.8 \cdot 0.99 \cdot 0.2 \\
&= 0.1584. 
\end{align*}
Doing these calculations for all cases, one will obtain
\begin{align*}
P(R = \rT | G = \gT) &= \frac{0.00198_{\rT, \sT, \gT} + 0.1584_{\rT, \sF, \gT} }{0.00198_{\rT, \sT, \gT} + 0.288_{\rF, \sT, \gT} + 0.1584_{\rT, \sF, \gT} + 0.0_{\rF, \sF, \gT}}\\
& = \frac{891}{2491} \approx 0.3577.
\end{align*}
Hence, the probability that it is raining, given the grass is wet is approximately 36\%.
\end{example}
The procedure described in Example \ref{ex:exact-marg} is called \textit{exact marginalization}. It illustrates that in order to do inference, there is not always need for an explicit joint distribution. Though, when we deal with more complex BNs, \textit{exact marginalization} is still vulnerable for an exponentially blow up of the number of computations to be executed. In general, all exact inference methods -- variable elimination, clique tree propagation, recursive conditioning et cetera -- have to execute computations that are exponential in the network's treewidth (a measure for graph complexity). To make all of those concepts detailed here is out of the scope of this thesis. Therefore, for a comprehensive discussion of those methods we refer to \cite{koller2009probabilistic}, Chapter~9; \cite{nielsen2009bayesian}, Chapter~4. \\
One way to avoid the exponential character of exact inference techniques, is to consider approximate inference methods. In order to do so, in the next section, we introduce how samples of states of BNs could be generated by using Markov chain Monte Carlo simulations.

%In cases where much of the evidence is at the leaves of the network, forward sampling techniques for example are essentially sampling from the prior distribution, which is often very far from the desired posterior. 

%\subsection{Exact algorithms}
%\subsection{Approximation algorithms}

%answer queries of the form

\section{Approximate sampling methods}\label{sec:approx-inf}
Approximate sampling methods extract characteristics of the BN by applying statistics on a large bunch of generated samples. These samples are generated according to a heuristic related to the BN structure and corresponding CPTs. In this section, we show how unobserved variables could be inferred based on this bunch of samples and which heuristic Metropolis- and Gibbs sampling use to create samples.\\ 

For now, consider we have a bunch of $6$ samples. The next example illustrates the key concept of all approximate inference techniques.
\begin{example}\label{ex:sampling}
Consider the BN in Figure \ref{rainBN}. Let \rr be our variable of interest and suppose $\bfx^{(1)}, \ldots , \bfx^{(6)}$ are independent and identically distributed (i.i.d.) samples of $(R, S, G)$ created by a certain sampling heuristic. The bunch of $6$ samples is given by
\begin{multicols}{2}
\begin{enumerate}
\item $\bfx^{(1)} = ( R^{(1)} = \rF, S^{(1)} = \sT,  G^{(1)} = \gT)$
\item $\bfx^{(2)} = ( R^{(2)} = \rT, S^{(2)} = \sF,  G^{(2)} = \gT)$
\item $\bfx^{(3)} = ( R^{(3)} = \rF, S^{(3)} = \sT,  G^{(3)} = \gT)$
\item $\bfx^{(4)} = ( R^{(4)} = \rF, S^{(4)} = \sT,  G^{(4)} = \gF )$
\item $\bfx^{(5)} = ( R^{(5)} = \rT, S^{(5)} = \sT,  G^{(5)} = \gT)$
\item $\bfx^{(6)} = ( R^{(6)} = \rF, S^{(6)} = \sF,  G^{(6)} = \gF)$.
\end{enumerate}
\end{multicols}
%Here, $\textasteriskcentered$ denotes that any value could be assigned to the corresponding variable, though (in this context) which values exactly is not relevant. 
\noindent Then, the probability of $R = \text{r}_\text{T}$ can be approximated by 
\begin{align*}
\widetilde{P}_6(R = \rT) = \E[ \mathds{1}_{R = \rT} ] = \sum_{t=1}^6 \mathds{1}_{R^{(t)} = \rT} = \frac{2}{6},
\end{align*}
where $\mathds{1}_{R = \rT}$ is the indicator function of the event $R = \rT$.
\end{example}
In general, for a variable of interest $X$ with $T \in \N$ (not te be confused with T  for true) i.i.d. samples the probability of $X = x$ can be approximated by 
\begin{align*}
\widetilde{P}_T(X = x) = \E[ \mathds{1}_{X=x} ] = \sum_{t=1}^T \mathds{1}_{X^{(t)} = x}.
\end{align*}
A plethora of techniques have been develop to create samples $\bfx^{(1)}, \ldots \bfx^{(T)}$. Again, discussing all those methods here in detail is beyond the scope of this thesis. For an overview of approximate sampling techniques and its limitations we refer to \cite{koller2009probabilistic}, Chapter 12; \cite{nielsen2009bayesian} Chapter 4. In the next subsection, we focus on how samples of a BN can be generated using Markov chain Monte Carlo methods. 

\subsection{Markov chain Monte Carlo sampling}
We introduce the concept of Markov chain Monte Carlo sampling and explain under which conditions the Markov chain guarantee convergence to the desired posterior distribution. Furthermore, we give an illustration how Metropolis sampling could be used on the \textit{Rain-Sprinkler} BN. \\

In contrast to Example \ref{ex:sampling}, suppose that sample $\bfx^{(t)}$ is not created i.i.d. for all $t \geq 1$. But we create sample $\bfx^{(t+1)}$ by tweaking the state of sample $\bfx^{(t)}$ according to a certain heuristic. Repeating this process yields the Markov chain $(\bfx^{(t)})_{t \in \N_0}$. Repeating this often -- for $t = 1, \ldots T$ with large $T$  -- this process is called \textit{Markov chain Monte Carlo} (MCMC) sampling. 
Due to a cleverly chosen heuristic, MCMC methods construct a Markov chain such that, although the first sample may be generated from the prior distributions, successive samples are generated from distributions that provably get closer and closer to the desired posterior distribution. It could be shown \citep[p.~517]{koller2009probabilistic} that $\widetilde{P}_T \to P$ as $T \to \infty$. In order to use this tempting feature of MCMC methods, we need to guarantee that the limit of this process exists and is unique. Since we only consider Markov chains on finite state spaces, from the theory of Markov chains we know that if a Markov chain is regular and reversible with respect to a distribution $\pi$, then $\pi$ is the unique stationary distribution. These notions are defined below.


%In cases where much of the evidence is at the leaves of the network, forward sampling techniques for example are essentially sampling from the prior distribution, which is often very far from the desired posterior. 

\begin{definition}[Regular Markov chain]
A Markov chain is said to be regular if there exists some number $k \in \N$ such that for every $\bfx, \bfx' \in Val(\bfX)$ the probability of getting from $\bfx$ to $\bfx'$ -- denoted as $(\bfx \to \bfx')$ -- in exactly $k$ steps, is $> 0$. 
\end{definition}

\begin{definition}[Reversible Markov chain]
A finite-state Markov chain $Q$ is called reversible if there exists a unique distribution $\pi$ such that for all states $\bfx$ and $\bfx'$
\begin{align}
\pi(\bfx) Q(\bfx \to \bfx') = \pi(\bfx') Q(\bfx' \to \bfx).
\end{align}
\end{definition}

\begin{definition}[Stationary distribution]
A distribution $\pi$ is a stationary distribution for a Markov chain Q if
\begin{align}
\pi(\bfx') = \sum_{\bfx \in Val(\bfX)} \pi(\bfx) Q(\bfx \to \bfx').
\end{align}
\end{definition}
A heuristic that generates a Markov chain that tend to be regular and reversible is \textit{Metropolis sampling}. In the context of BN sampling, it follows the next procedure.
\begin{definition}[Metropolis sampling]
\leavevmode
\makeatletter
\@nobreaktrue
\makeatother
\vspace{0.5pc}
\begin{itemize}
\item Select an initial state $\bfx^{(0)}$ of the BN;
\item for each iteration $0 < t \leq T$: create a candidate sample $\bfx'$ by drawing a value from a proposed distribution. Here, for all $X_i$ we draw uniformly random a value from $Val(X_i)$;
%We could consider a normal distribution being `centered' on $X_i = x_i$ as making a random one-to-one representation of $Val(X_i) = \{x_1, \ldots , x_m \}$ (according to the fixed ordening) to the set $\{- \floor*{\frac{m}{2}}, \floor*{\frac{m}{2}}\}$. Then, we sample a random number $u$ from a normal distribution with mean that corresponds to the integer representing the variable of interest. According to specific boundaries we end up in a new proposed state.
\item determine the acceptance ratio $\alpha = \min \big( 1, \frac{P(\bfx')}{P(\bfx^{(t-1)})} \big)$ from the CPTs;
\item generate a uniform random number $u \in [0,1]$;
\begin{itemize}
\item if $u \leq \alpha$ then $\bfx^{(t)} = \bfx'$;
\item if $u > \alpha$ then $\bfx^{(t)} = \bfx^{(t-1)}$.
\end{itemize}
\end{itemize}
The above construction allows us to produce a Markov chain for an arbitrary stationary distribution. Though, in order to guarantee convergence to the desired distribution, we point out that the constructed chain still needs to be regular. This property does not follow directly from the construction. Under which conditions Metropolis sampling guarantees convergence is discussed in \citep[p.~505]{koller2009probabilistic}.
\end{definition}
In the next example, we show how Metropolis sampling works out on the \textit{Rain-Sprinkler} network.
\begin{example}[Metropolis sampling]
Consider the BN in Figure \ref{rainBN}. Suppose that we have the initial state 
\begin{align*}
\bfx^{(0)} = (R^{(0)} = \rT, S^{(0)} = \sF, G^{(0)} = \gT)
\end{align*}
and no evidence is available. We select $R^{(1)}$ according the distribution $P(R^{(1)}=\rT)=P(R^{(1)}=\rF) = 0.5$. Repeating this for $S$ and $G$, one could obtain
%In order to generate a sample $R^{(1)}$, according to a normal distribution, we could consider a normal distribution being `centered' on $R^{(0)} = \rT$ as making a one-to-one representation of $Val(R) = \{\rT, \rF\}$ to the set $\{0, 1\}$. Then, we can sample a random number $u$ according a standard normal distribution. Then, if $u \leq 0$: $X^{(1)} = \rT$ and if $u > 0: X^{(1)} = \rF$. 
\begin{align*}
\bfx' = (R' = \rT, S' = \sF, G' = \gF).
\end{align*}
We determine $\alpha$ as
\begin{align*}
\alpha = \frac{P(R' = \rT, S' = \sF, G' = \gF)}{P(R^{(0)} = \rT, S^{(0)} = \sF, G^{(0)} = \gT)} = \frac{0.0396}{0.1584} = 0.25. 
\end{align*}
Then, we $\bfx^{(1)} = \bfx'$ if the uniform random number $u \in [0,1]$ is smaller than $0.25$, otherwise $\bfx^{(1)} = \bfx^{(0)}$.
\end{example}
Repeating this procedure, we are able to generate $T$ samples of the BN. Ultimately, from this bunch of samples we can do inference as described in Example \ref{ex:sampling}. When we adjust the proposal distribution of Metropolis sampling, we could obtain a special case of Metropolis sampling called \textit{Gibbs sampling}. 
\subsection{Gibbs sampling}
Gibbs sampling \cite{geman1984stochastic} is one of the most popular MCMC methods to date.  We show how Gibbs sampling is related to Metropolis sampling and how regularity and reversibility could break down due to the appearance of deterministic relations in the BN.
\begin{definition}[Gibbs sampling]
Let $(X_1, \ldots , X_n)$ be an arbitrary ordering of the variables in $\G$. The Gibbs sampling algorithm begins with a random assignment $\bfx^{(0)}$ to all variables in the BN. Then, for $t = 1, \ldots, T$ it performs the following $T$ Gibbs iterations. For $i=1, \ldots, n$, it generates a new value $x_i^{(t)}$ for variable $X_i$ by sampling a value from the distribution $P(X_i | \bfx_{-i}^{(t)})$, where $\bfx_{-i}^{(t)} = (x_1^{(t)}, \ldots, x_{i-1}^{(t)}, x_{i+1}^{(t-1)}, \ldots, x_{n}^{(t-1)})$. After $T$ samples are generated, all one-variable marginals can be estimated using the following quantity
\begin{align}\label{eq:gibss}
\widetilde{P}_T(x_i) = \frac{1}{T} \sum_{t=1}^T P(x_i | \bfx_{-i}^{(t)} ).
\end{align}
It could be shown, that in the limit of infinite samples, $\widetilde{P}_T(x_i)$ will converge to $P(x_i)$ if the underlying Markov chain is regular and reversible \cite{venugopal2013giss}.
\end{definition}
In the next example, we show how Gibbs sampling works out on the \textit{Rain-Sprinkler} network.
\begin{example}[Gibbs sampling]
Consider the BN in Figure \ref{rainBN}. Suppose we have the initial state 
\begin{align*}
\bfx^{(0)} = (R^{(0)} = \rT, S^{(0)} = \sF, G^{(0)} = \gT)
\end{align*}
and no evidence is available. For each Gibbs iteration, we resample all unobserved variables, one in a time, in a predetermined order, say $(R, S, G)$. So, we first sample $R^{(1)}$ from the distribution $P(R | S^{(0)} = \sF, G^{(0)} = \gT)$, which could be made explicitly
%\begin{align*}
%P(R = \rT | S^{(0)} = \sT, G^{(0)} = \gT) &= \frac{P(\rT, \sT,  \gT)}{P(\sT, \gT)} = \frac{P(\gT | \rT, \sT) P(\sT | \rT) P(\rT)}{\sum_{\rr \in {\{T,F\}}}P(\rr, \sT, \gT)} \\
%&= \frac{0.99 \cdot 0.01 \cdot 0.2}{0.00198_{\rT, \sT, \gT} + 0.288_{\rF, \sT, \gT}} \approx 0.0068.
%\end{align*}
\begin{align*}
P(R = \rT | S^{(0)} = \sF, G^{(0)} = \gT) &= \frac{P(\rT, \sF,  \gT)}{P(\sT, \gT)} = \frac{0.1584}{0.00198_{\rT, \sT, \gT} + 0.288_{\rF, \sT, \gT}} \approx 0.5462.
\end{align*}
Hence $P(R = \rF | S^{(0)} = \sF, G^{(0)} = \gT) \approx 0.4538$. If we have drawn a value from this distribution, for example $R^{(1)} = \rF$, we continue to resample $S^{(1)}$ from the distribution $P(S | R^{(1)} = \rF, G^{(0)} = \gT)$, obtaining for example $S^{(1)} = \sT$. Finally, we sample $G^{(1)}$ from $P(G | R^{(1)} = \rF, S^{(1)} = \sT)$, resulting in $\gT$. Therefore, the result of the first iteration of Gibbs sampling is the sample $\bfx^{(1)} = (R^{(1)} = \rF, S^{(1)} = \sT, G^{(1)} = \gT)$. This process repeats.
\end{example}
As mentioned before, Gibbs sampling could perform poorly in the presence of deterministic relations \cite{koller2009probabilistic, poon2006sound, gogate2011samplesearch}. In the next example, we give an illustration of this phenomenon.
\begin{example}\label{ex:gibbs}
Consider the BN in Figure \ref{gibbs}. Suppose the initial configuration $\bfx^{(0)} = (A^{(0)} = 0,\ B^{(0)} = 0)$ -- shortened $(0,0)$ -- is given and suppose no evidence is available. 
In the first Gibbs iteration, we resample both unobserved variables, one at a time, in the order $A, B$. Thus, we first sample $A^{(1)}$ from the distribution $P(A | B^{(0)} = 0)$. According to the CPT, with probability $1$ this turns out to be $A=0$. Consecutively, we sample $B^{(1)}$ from the distribution $P(B | A^{(1)} = 0 )$. Which always returns $B = 0$. As a consequence, the Markov chain created by Gibbs sampling behaves like
\begin{align*}\label{gibbs-trap}
(0,0) \to (0,0) \to (0,0) \to \ldots ,
\end{align*}
yielding that $\bfx^{(i)} = (0,0)$ for all $i \geq 0$. Hence, inference based on Gibbs sampling returns $P(A=0) = 1$. The real distribution for $A$ on the other hand must equal $P(A=0) = P(A=1) = 0.5$.
\end{example}
When no deterministic relations are present in a BN, it could be shown \cite{poon2006sound} that Gibbs sampling generates a regular and reversible Markov chain (and therefore converges to the desired posterior distribution). Though, as illustrated in Example \ref{ex:gibbs}, when deterministic dependencies are present in the BN, regularity and reversibility could break down and the estimation given in Equation (\ref{eq:gibss}) could no longer converge to $P(x_i)$. \\
Many solutions have been proposed in the past to address this problem \cite{venugopal2013giss, poon2006sound}. So do we: we devised a MCMC method that always converges to the desired posterior distribution, especially in the presence of deterministic relations.
\begin{center}
\begin{figure}[h!]
\centering
\begin{tikzpicture}[node distance=1.2cm, >=triangle 60]
\node [events] (a) {A};
\node [events,  right=of a] (b) {B};


\draw [->] (a) -- (b);

{\small
\node [left = 1cm of a] (ta) {
    \begin{tabular}{|c||c|} \hline
    	$A=0$ & $0.5$ \\ \hline
    	$A=1$ & $0.5$\\ \hline
    \end{tabular}
    };
    
\node [right = 1cm of b] (tb) {
    \begin{tabular}{|c||c|c|} \hline
	&$A=0$&$ A=1 $  \\ \hline \hline
	$B=0$ & $1$ & $0$ \\ \hline
	$B=1$ & $0$ & $1$ \\ \hline
\end{tabular}
    };}
    
\draw [dotted] (a) -- (ta);
\draw [dotted] (b) -- (tb);
\end{tikzpicture}
\caption{a BN with a deterministic relation. The state of B is equal to the state of A with probability 1.}
\label{gibbs}
\end{figure}
\end{center}

%Despite that Gibbs sampling converges often to the desired posterior, reversibility and regularity could not always be guaranteed.

\chapter{Prune Sampling}
\section{The prune sampling algorithm}
\subsection{Background: MC-SAT algorithm}
\subsection{Notation and definition}


\begin{figure}[h!]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tikzpicture}[node distance=1.2cm, >= triangle 60]

\node [events] (kidney) {Kidney};
\node [events, below right=of kidney] (bp) {BloodPres.};
\node [events, above right=of bp] (lifestyle) {Lifestyle};
\node [events, below=of bp] (measure) {Measurement};
\node [events, below right=of lifestyle] (sports) {Sports};


\draw [->] (kidney) -- (bp);
\draw [->] (lifestyle) -- (bp);
\draw [->] (lifestyle) -- (sports);
\draw [->] (bp) -- (measure);

{\small
\node [left = 1cm of kidney] (t_k) {
    \begin{tabular}{|c||c|} \hline
    	$k_b$ & $\cancel{0.5}$\\ \hline
    	$\mathbf{k_g}$ & $\mathbf{0.5}$ \\ \hline
    \end{tabular}
    };
    
\node [right = 1cm of lifestyle] (t_l) {
\begin{tabular}{|c||c|} \hline
	$\mathbf{l_b}$ & $\mathbf{0.5}$\\ \hline
	$l_g$ & $0.5$ \\ \hline
\end{tabular}
};
    
\node [left = 1cm of bp] (t_bp) {
    \begin{tabular}{|c||c|c|c|c|} \hline
	&$k_b, l_b$ & $ k_b, l_g $ &$ \mathbf{k_g, l_b}$ & $ k_g, l_g $  \\ \hline \hline
	$b_n$ & $\cancel{0.1}$ & $\cancel{0.2}$ & $\cancel{0.2}$ & $0.9$  \\ \hline
	$\mathbf{b_e}$ & $0.9$ & $0.8$ & $\mathbf{0.8}$ & $\cancel{0.1}$ \\ \hline
\end{tabular}};

\node [right = 0.6cm of sports] (t_s) {
    \begin{tabular}{|c||c|c|} \hline
	&$\mathbf{l_b}$&$ l_g $  \\ \hline \hline
	$s_n$ & $0.8$ & $\cancel{0.2}$  \\ \hline
	$\mathbf{s_y}$ & $\mathbf{0.2}$ & $\cancel{0.8}$ \\ \hline
\end{tabular}};

\node [right = 1cm of measure] (t_m) {
    \begin{tabular}{|c||c|c|} \hline
	&$b_n$&$ \mathbf{b_e} $  \\ \hline \hline
	$m_n$ & $0.9$ & $\cancel{0.1}$  \\ \hline
	$\mathbf{m_e}$ & $\cancel{0.1}$ & $\mathbf{0.9}$ \\ \hline
\end{tabular}}

;}
    
\draw [dotted] (kidney) -- (t_k);
\draw [dotted] (lifestyle) -- (t_l);
\draw [dotted] (bp) -- (t_bp);
\draw [dotted] (sports) -- (t_s);
\draw [dotted] (measure) -- (t_m);
\end{tikzpicture}
\end{adjustbox}
\caption{a pruned version of the BloodPressure network around the boldfaced initial state $\bfx = (k_g, l_b, b_e, s_y, m_e)$. Note that the lower the value of the CPT-entry, the higher the probability that the index gets pruned. We see that $S_{\C_\bfx^{\text{np}}}$ contains two feasible states, i.e. $(k_g, l_b, b_e, s_y, m_e)$ and $(k_g, l_b, b_e, s_n, m_e)$.}
\label{pruning}
\end{figure}

\begin{example}\label{ex:pruning}
Consider the BN in Figure \ref{pruning}. Pruning around the boldfaced initial state $\bfx = (k_g, l_b, b_e, s_y, m_e)$ could yield the non-crossed indices 
\begin{align*}
\C_\bfx^{\text{p}} = \{ K(2), L(2), BP(4), BP(5), BP(6), S(1), M(1) \}.
\end{align*}
Note that the lower the value of the CPT-entry, the higher the probability that the index gets pruned. 
\end{example}

\begin{algorithm}[h!]
\renewcommand\thealgorithm{1}
\caption{Prune sampling algorithm}
\label{prunealg}
\begin{algorithmic}
%\Require{We can prune a BN given a state $\mathbf{x}$}
\Function{PruneSampling}{BN, initial, M}
\State $\mathbf{x}^{(0)} \gets $ initial
     \State $\mathcal{S} \gets \{\mathbf{x}^{(0)}\}$
     \For{$m \gets 1 $ to$ $ M}
     \State $\C_{\bfx^{(m-1)}}^{\text{p}} \gets \text{Prune around } \mathbf{x}^{(m-1)}$ \\ \Comment{{\footnotesize See Definition $3.2$ }}
	\State $\C_{\bfx^{(m-1)}}^{\text{np}} \gets \mathcal{C} \setminus \C_\bfx^{\text{p}}$  
     \State $\mathbf{x}^{(m)} \sim  \U(S_{\C_\bfx^{\text{np}}}) $ 
     \State $\mathcal{S} \gets \mathcal{S} \cup \mathbf{x}^{(m)}$
     \EndFor
     \State \Return{$\mathcal{S}$}
\EndFunction
\end{algorithmic}
\end{algorithm}



\subsection{Regularity and reversibility}

\section{Practical implementation}\label{sec:prune_2}
\subsection{Generate initial states}
\subsection{Sampling from the pruned network}

\chapter{Results}

\section{Simple deterministic network}

\begin{figure}[h]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/simple_det_250_samples.png}
  \caption{250 samples prune vs Gibbs}
  \label{fig:sub1}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/simple_det_10000_samples.png}
  \caption{10.000 samples prune vs Gibbs}
  \label{fig:sub2}
\end{subfigure}
\caption{illustration of superior performance of \ps. Due to the deterministic relation in the BN given in Example 2.1, Gibbs sampling is trapped in the subset $\{0\}$ and $\{1\}$ of the state space $\{0,1\}$. Hence, approximating $P(A = 0)$ as $0$ or $1$ (red and green line). As a consequence of the regular and reversible Markov chain generated by pruning (blue and orange line), this Markov chain is able to move around the entire state space and therefore converges to the correct probability $P(A = 0) = 0.5$.}
\label{simple-deterministic}
\end{figure}

\newpage
\subsection{Block shaped network}

\begin{figure}[h]
\centering
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/gibbs_trap_2_gibbs_vs_2_prune_50_samples.png}
  \caption{50 samples prune vs Gibbs}
  \label{fig:sub1}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
  \centering
  \includegraphics[width=\linewidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/gibbs_trap_2_gibbs_vs_2_prune_10000_samples.png}
  \caption{10.000 samples prune vs Gibbs}
  \label{fig:sub2}
\end{subfigure}
\vspace{0.75pc}
\caption{\ps could be superior to Gibbs sampling, even in the absence of deterministic relations. A non-deterministic block shaped CPT -- as presented in Table \ref{block} -- can prevent a Markov chain generated by Gibbs sampling of visiting the entire state space. In this example, being trapped in the subset $\{0, 1\}$ or $\{2, 3\}$ both yield the probability $0.5$ of assigning $0,1$ respectively $2,3$ to variable $X_i$ (red and green line). \textit{Prune sampling} generates a Markov chain that is regular and reversible and therefore can move around freely through the whole state space. Hence converging to the uniformly probability $0.25$ of assigning value $0,1,2$ or $3$ to variable $X_i$ (blue and orange line).} 
\label{block-BN}
\end{figure}

\vspace{2pc}
$P(X_i |X_{i-1})=$
\begin{table}[h]
\centering
\resizebox{0.5\columnwidth}{!}{
\begin{tabular}{|c||c|c|c|c|} \hline
	&$X_{i-1}=0$ & $ X_{i-1}=1 $ & $X_{i-1}=2$ & $X_{i-1}=3$  \\ \hline \hline
	$X_i =0$ & $0.5$ & $0.5$ & $0$ & $0$ \\ \hline
	$X_i =1$ & $0.5$ & $0.5$ & $0$ & $0$ \\ \hline
	$X_i =2$ & $0$ & $0$ & $0.5$ & $0.5$ \\ \hline
	$X_i =3$ & $0$ & $0$ & $0.5$ & $0.5$ \\ \hline
\end{tabular}
}.
\caption{a block shaped CPT}
\label{block}
\end{table}

\section{Benchmark Bayesian networks}
\subsection{Accuracy}
\subsubsection{Real world Bayesian networks 0\% determinism}\label{real_world_no_evidence}

\begin{figure}[H]
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/asia_ahd_25000.png}
\caption{Asia}%
\label{asia}%
\end{subfigure}\hfill%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/alarm_ahd_25000.png}
\caption{Alarm}%
\label{alarm}%
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/win95pts_ahd_25000.png}
\caption{Win95pts}%
\label{win95pts}%
\end{subfigure}\hfill%
\vspace{0.75pc}
\caption{underperformance of \ps in terms of accuracy on the benchmark real world BNs without any given evidence. Since no evidence is available, 0\% of the nodes contain deterministic relations. Because \ps is created to deal with determinism, we did had expected underperformance of the pruning technique on these types of networks.  }
\label{results2}
\end{figure}

\newpage
\subsubsection{Real world Bayesian networks 25\% determinism}

\begin{figure}[H]
\centering
\begin{subfigure}{0.5\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/asia_ev_ahd_25000.png}
\caption{Asia}%
\label{asia}%
\end{subfigure}\hfill%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/alarm_ev_ahd_25000.png}
\caption{Alarm}%
\label{alarm}%
\end{subfigure}%
\begin{subfigure}{0.5\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/win95pts_ev_ahd_25000.png}
\caption{Win95pts}%
\label{win95pts}%
\end{subfigure}\hfill%
\vspace{0.75pc}
\caption{compared with the results in section \ref{real_world_no_evidence}, \ps starts to become more competitive in terms of accuracy as more determinism is present in the BNs. In these real world BNs with evidence, 25\% of the nodes contain deterministic relations. Though, on the win95pts BN \ps underperforms significantly.}
\label{results2}
\end{figure}

\newpage
\subsubsection{Grid Bayesian networks 50\% determinism}
\begin{figure}[H]
\centering
\begin{subfigure}{0.49\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/grid_3x3_ahd_25000.png}
\caption{The average Hellinger distance (AHD) between the exact and the approximate one-variable marginal $X 3\_3$ in the Grid 3x3 network. }%
\label{grid_3x3}%
\end{subfigure}\hfill%
\begin{subfigure}{0.49\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/grid_5x5_ahd_25000.png}
\caption{The average Hellinger distance (AHD) between the exact and the approximate one-variable marginal $X 5\_5$ in the Grid 5x5 network.}%
\label{grid_5x5}%
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
\includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/grid_8x8_ahd_25000.png}
\caption{The average Hellinger distance (AHD) between the exact and the approximate one-variable marginal $X 8\_8$ in the Grid 8x8 network.}%
\label{grid_8x8}%
\end{subfigure}\hfill%
\caption{underperformance of \ps in terms of accuracy on the benchmark grid BNs. In these grid networks 50\% of the nodes are deterministic. Because \ps is created to deal with determinism, we did expected that -- due to the ubiquity of determinism -- \ps would perform much better. Though, in presence of much deterministic relations, there is a risk Gibbs and Metropolis sampling do not converge to the correct posterior distribution. Though, if Gibbs/Metropolis sampling do converge -- as above -- to the correct distribution, \ps is less accurate.}
\label{results1}
\end{figure}


\subsection{Rate of convergence}

\begin{figure}[H]
\centering
\begin{subfigure}[t]{0.5\textwidth}
  \centering
  \captionsetup{width = 0.9\textwidth}
  \includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/1.png}
  \caption{$100$ Metropolis runs of $25.000$ samples approximate the one-variable marginal $X 8\_8$ of the Grid $8 \times 8$ BN as $0.81$.}
  \label{sub_a}
\end{subfigure}%
\begin{subfigure}[t]{0.5\textwidth}
  \centering
  \captionsetup{width = 0.9\textwidth}
  \includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/2.png}
  \caption{According to convergence class $\mathcal{O}(t^{-1/2})$, the standard deviation ${\sigma}^2(t)$ of the $100$ Metropolis samples decreases with the number of samples $t$ with rate $t^{-1/2}$.}
  \label{sub_b}
\end{subfigure}

\begin{subfigure}[t]{0.5\textwidth}
  \centering
  \captionsetup{width = 0.9\textwidth}
  \includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/3.png}
  \caption{Plotting the $\log$ of the standard deviation -- $\log {\sigma}^2(t)$ -- versus the $\log$ of the number of points -- $\log t$ -- helps us to estimate the proportionality constant $\alpha$ in $\frac{\alpha}{\sqrt{t}}$. In this plot we have estimated $\alpha = 0.4$.}
  \label{sub_c}
\end{subfigure}%
\begin{subfigure}[t]{0.5\textwidth}
  \centering
  \captionsetup{width = 0.9\textwidth}
  \includegraphics[width = \textwidth]{/Users/jurriaan/Documents/LaTeX/MSc_thesis_UU/Paper_Prune_Sampling/4.png}
  \caption{To determine $\alpha$ based on its asymptotic behavior as $\alpha / \sqrt{t}$, we want to ignore the interval $10^0$ - $10^1$ in Figure \ref{sub_c}. In doing so, we introduce a polynomial expansion to approximate the linear log plot as $\alpha(1+\beta t^{-\delta})$. Fitting this function to the blue line in the above figure, we retrieve that $\alpha \approx 0.4$. }
  \label{sub_d}
\end{subfigure}
\caption{procedure to determine the proportionality constant of the sampling methods.}
\label{determine-c}
\end{figure}

\begin{center}
\begin{table}[H]
\begin{center}
\begin{tabular}{l c c c}  
\toprule
\multicolumn{4}{r}{Sampling method} \\
\cmidrule(r){2-4}
Bayesian \\ network    & Gibbs    & Metropolis & Prune  \\
\midrule
Asia\_ev0 & 0.52 & \textbf{0.51} & 0.81  \\
Alarm\_ev0 & \textbf{0.43} & 0.46 & 0.79  \\
Win95pts\_ev0 & \textbf{0.48} & 0.55 & 0.59  \\
Asia\_ev25 & 0.47 & \textbf{0.45} & 0.77  \\
Alarm\_ev25 & 0.40 & \textbf{0.37} & 0.49  \\
Win95pts\_ev25 & \textbf{0.45} & 0.48 & 0.50  \\
%SAM_vAN & 0.40 & 0.45 & 0.56  \\
Grid 3x3 & \textbf{0.37} & 0.38 & 0.62  \\
Grid 5x5 & 0.54 & \textbf{0.49} & 0.55  \\
Grid 8x8 & \textbf{0.39} & 0.40 & 0.53  \\
%Grid 11x11 & 0.45 & 0.45 & x \\
\bottomrule
\end{tabular}
\caption{the proportionality constants of \ps are always higher than Gibbs- and Metropolis sampling on the benchmark BNs. Hence, samples generated by \ps converge slower to the desired distribution.}
\label{ROC-table}
\end{center}
\end{table}
\end{center}

\subsection{Time consumption}

\begin{center}
\begin{table}[!htb]
\begin{center}
\begin{tabular}{l c c c}  
\toprule
\multicolumn{4}{r}{Sampling method} \\
\cmidrule(r){2-4}
Bayesian \\ network    & Gibbs    & Metropolis & Prune  \\
\midrule
Asia\_ev0 & 2.72 & 1.31 & \textbf{0.53}  \\
Alarm\_ev0 & 12.27 & 3.94 & \textbf{3.56}  \\
Win95pts\_ev0 & 36.38 & \textbf{21.32} & 49.85  \\
Asia\_ev25 & 2.56 & 1.05 & \textbf{0.41}  \\
Alarm\_ev25 & 9.92 & 3.07 & \textbf{2.83}  \\
Win95pts\_ev25 & 29.65 & \textbf{16.03} & 40.03  \\
Grid 3x3 & 1.67 & 1.76 & \textbf{0.73}  \\
Grid 5x5 & 12.13 & 4.70 & \textbf{2.42}  \\
Grid 8x8 & 20.50 & \textbf{8.83} & 105.17  \\
\bottomrule
\end{tabular}
\caption{time consumption of the sampling methods. \Ps is the fastest on small and medium sized network. Metropolis performs better on the large sized networks. }
\label{ROC-table}
\end{center}
\end{table}
\end{center}

%\section{Measures of complexity}
%\subsection{d-seperation}
%\subsection{Treewidth}
%\subsection{Junction tree algorithm}
%
%
%\chapter{Improvement of prune sampling}
%\section{Logic sampling}
%\section{Simulated annealing}
\chapter{Improvement of prune sampling}
\section{Uniformly sampling}
\begin{figure*}[h!]
\centering
\captionsetup[subfigure]{justification=centering}

\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.2\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Asia/Prune/asia_hist_num_samples.png}
\caption{Asia 0\% determinism, max set size $16$}%
\label{asia_ev}%
\end{subfigure}\hfill%
\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.2\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Alarm/Prune/alarm_hist_num_samples.png}
\caption{Alarm 0\% determinism, max set size $2.218$}%
\label{alarm_ev}%
\end{subfigure}\hfill%
\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.2\columnwidth]{example-image-c}
\caption{Win95pts 0\% determinism, max set size $34.125$}%
\label{win95pts_ev}%
\end{subfigure}\hfill%

\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.1\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Asia_ev25/Prune/asia_ev_hist_num_samples.png}
\caption{Asia 25\% determinism, max set size $8$}%
\label{asia_ev}%
\end{subfigure}\hfill%
\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.2\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Alarm_ev25/Prune/alarm_ev_hist_num_samples.png}
\caption{Alarm 25\% determinism, max set size $505$}%
\label{alarm_ev}%
\end{subfigure}\hfill%
\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.2\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Win95pts_ev25/Prune/win95pts_ev_hist_num_samples.png}
\caption{Win95pts 25\% determinism, max set size $\ldots$}%
\label{win95pts_ev}%
\end{subfigure}\hfill%

\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.2\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Grid_3x3/Prune/grid_3x3_hist_num_samples.png}
\caption{Grid 3x3 50\% evidence, max set size $14$}%
\label{grid_3x3}%
\end{subfigure}\hfill%
\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.2\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Grid_5x5/Prune/grid_5x5_hist_num_samples.png}
\caption{Grid 5x5 50\% evidence}%
\label{grid_5x5, max set size $130$}%
\end{subfigure}\hfill%
\begin{subfigure}{.3\linewidth}
\includegraphics[width=1.1\columnwidth]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Grid_8x8/Prune/grid_8x8_hist_num_samples.png}
\caption{Grid 8x8 50\% evidence}%
\label{grid_8x8}%
\end{subfigure}\hfill%

\vspace{0.75pc}
\caption{histogram of the number of feasible states in the pruned network. As the size of the BNs (nodes and parameters) increases, the number of feasible states increases. Therefore, exhaustive listing of all feasible states becomes time intensive for big BNs.}
\label{results1}
\end{figure*}


\section{Pre-determined fixed set size}
\begin{figure*}[h!]
\centering

\begin{subfigure}{.5\linewidth}
\includegraphics[height=5.3cm]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Alarm/alarm_ahd4_25000.png}
\caption{Alarm 0\% determinism}%
\label{alarm_ev}%
\end{subfigure}\hfill%
\begin{subfigure}{.5\linewidth}
\includegraphics[height=5.3cm]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Win95pts/win95pts_ahd4_25000.png}
\caption{Win95pts 0\% determinism}%
\label{win95pts_ev}%
\end{subfigure}

\begin{subfigure}{.5\linewidth}
\includegraphics[height=5.3cm]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Alarm_ev25/alarm_ev_ahd4_25000.png}
\caption{Alarm 25\% determinism}%
\label{grid_3x3}%
\end{subfigure}\hfill%
\begin{subfigure}{.5\linewidth}
\includegraphics[height=5.3cm]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Win95pts_ev25/win95pts_ev_ahd7_25000.png}
\caption{Win95pts 25\% determinism}%
\label{grid_5x5}%
\end{subfigure}

\begin{subfigure}{.5\linewidth}
\includegraphics[height=5.3cm]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Grid_5x5/grid_5x5_ahd6_25000.png}
\caption{Grid 5x5 50\% evidence}%
\label{grid_3x3}%
\end{subfigure}\hfill%
\begin{subfigure}{.5\linewidth}
\includegraphics[height=5.3cm]{/Users/jurriaan/Documents/MScWiskunde/20180907_TNOdrive/Bayesian_network_repository/xdsl-benchmark-GeNIe-files/Grid_8x8/grid_8x8_ahd7_25000.png}
\caption{Grid 8x8 50\% evidence}%
\label{grid_5x5}%
\end{subfigure}

\vspace{0.75pc}
\caption{consistent underperformance of \ps using the hybrid forward sampling approach, i.e. to construct a set $V$ (of predetermined fixed size $1$, $10$, $100$ or $1000$) of feasible states in the pruned BN. }
\label{results1}
\end{figure*}

\chapter{Conclusion}
\citep{lauritzen1988local}
\gls{var_set}
\gls{var}

%List of abbreviations
\printglossaries

%Bibliography
\bibliographystyle{acm}
\bibliography{ref_msc_thesis}


\end{document}